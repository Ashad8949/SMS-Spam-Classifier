{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdaqf6vgTTg4"
   },
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2VgPk2mTThF"
   },
   "source": [
    "# **SPAM classification task**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkRBtAnQTThK"
   },
   "source": [
    "The purpose of this lab is to build Machine & Deep Learning models at the interface of `NLP` and `Network Security` areas through the use of `SMS Spam Collection dataset` with helping frameworks & libraries.\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "1.  Quickly explore the SMS Spam Collection dataset and build the best models with the help of functional programming and layer-by-layer model description to solve a SPAM classification task.\n",
    "2.  Show different calculated metrics of the built models.\n",
    "3.  Change values of some hyperparameters for model training process improving to achieve better results.\n",
    "4.  Visualize the data analysis results with various plot types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHwOXFUpTThK"
   },
   "source": [
    "## Agenda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHSc4iksTThP"
   },
   "source": [
    "*   Theory and Methods\n",
    "*   General part\n",
    "    *   Import required libraries and dataset\n",
    "    *   Some additional & preparing actions & add functions\n",
    "    *   Reading the Dataset\n",
    "    *   Dataset manipulations & simple EDA\n",
    "    *   Dataset size & feature names\n",
    "    *   Dataset primary statistics\n",
    "    *   Part A. Advanced Machine Learning for SPAM classification task\n",
    "    *   Part B. Advanced Deep Learning for SPAM classification task\n",
    "*   Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4t5rpwITThQ"
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoNAvKmglBm-"
   },
   "source": [
    "## Theory and Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOgyr5RSlI40"
   },
   "source": [
    "The basics of natural language processing (NLP)\n",
    "\n",
    "The data that we are going to use for this is a subset of an open source default of `SMS Spam Collection dataset`, which contains SMS text examples and its corresponding labels (or tags: `Spam` and `Ham`). The file contains one message per line. Each line consists of two columns: v1 contains the label (`ham` or `spam`) and v2 contains the raw text.\n",
    "\n",
    "This corpus has been collected from free or free for research sources on the Internet:\n",
    "\n",
    "*   A collection of 425 SMS spam messages was manually extracted from the Grumbletext website. This is a UK forum where cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of spam messages texts in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext website is: [Web Link](http://www.grumbletext.co.uk/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01).\n",
    "*   A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link](http://www.comp.nus.edu.sg/\\~rpnlpir/downloads/corpora/smsCorpus/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01).\n",
    "*   A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis is available at [Web Link](http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01).\n",
    "*   Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link](http://www.esp.uem.es/jmgomez/smsspamcorpus/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01).\n",
    "\n",
    "The original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01). The creators would like to note that in case you find the dataset useful, please, make a reference to the previous paper and the [web page](http://www.dt.fee.unicamp.br/\\~tiago/smsspamcollection/) in your papers, research, etc.\n",
    "\n",
    "This work presents a number of statistics, studies and baseline results for a few machine learning methods.\n",
    "\n",
    "***\n",
    "\n",
    "Almeida, T.A., GÃ³mez Hidalgo, J.M., Yamakami, A. Contributions to the Study  of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11), Mountain View, CA, USA, 2011.\n",
    "\n",
    "***\n",
    "\n",
    "In addition, we will build a visualization of our results, specifically the obtained metrics (accuracy and loss), to choose the best model for further saving and forecasting based on this saved model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS8UOiQOTThR"
   },
   "source": [
    "## Import required libraries and dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr92Q0Cd3d7f"
   },
   "source": [
    "You can find the main file of the dataset by this link: [https://www.kaggle.com/uciml/sms-spam-collection-dataset?select=spam.csv](https://www.kaggle.com/uciml/sms-spam-collection-dataset?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01&select=spam.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BnB2tiSZh4R"
   },
   "source": [
    "Alternative URL for downloading of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ORfD1EUiZuLK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Advanced_ML_DL_spam_classification_L4/spam.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGH5DhdLHmOp"
   },
   "source": [
    "Import the necessary libraries to use in this lab. We can add some aliases (such as pd, plt, np, tf) to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkSUK1leF9Lg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from click->nltk) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from importlib-metadata->click->nltk) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from importlib-metadata->click->nltk) (4.3.0)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (4.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashad alam\\.conda\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install tensorflow==2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdVcijxmTTha"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import nltk, re, collections, pickle, os # nltk - Natural Language Toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# %matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.style.use('ggplot')\n",
    "seed = 42\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "warnings.simplefilter(action = 'ignore', category = Warning)\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Odfp7MOVWLKQ"
   },
   "source": [
    "## Some additional & preparing actions & add functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evtbog-C13Z2"
   },
   "source": [
    "Specify the value of the `precision` parameter equal to 3 to display three decimal signs (instead of 6 as default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwMoaJLV13Z3"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"precision\", 3)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW7wprNfJprA"
   },
   "source": [
    "Add some functions that you will need futher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlvlUEQ2PaKz"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'accuracy' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'accuracy' in s and 'val' in s]\n",
    "    \n",
    "    plt.figure(figsize = (12, 5), dpi = 100)\n",
    "    COLOR = 'gray'\n",
    "    \n",
    "    plt.rc('legend', fontsize = 14)   # legend fontsize\n",
    "    plt.rc('figure', titlesize = 12)  # fontsize of the figure title\n",
    "        \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1, len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.subplots_adjust(wspace = 2, hspace = 2)\n",
    "    plt.rcParams['text.color'] = 'black'\n",
    "    plt.rcParams['axes.titlecolor'] = 'black'\n",
    "    plt.rcParams['axes.labelcolor'] = COLOR\n",
    "    plt.rcParams['xtick.color'] = COLOR\n",
    "    plt.rcParams['ytick.color'] = COLOR\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b-o',\n",
    "                 label = 'Train (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g',\n",
    "                 label = 'Valid (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(facecolor = 'gray', loc = 'best')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.subplots_adjust(wspace = 2, hspace = 2)\n",
    "    plt.rcParams['text.color'] = 'black'\n",
    "    plt.rcParams['axes.titlecolor'] = 'black'\n",
    "    plt.rcParams['axes.labelcolor'] = COLOR\n",
    "    plt.rcParams['xtick.color'] = COLOR\n",
    "    plt.rcParams['ytick.color'] = COLOR\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b-o',\n",
    "                 label = 'Train (' + str(format(history.history[l][-1],'.4f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g',\n",
    "                 label = 'Valid (' + str(format(history.history[l][-1],'.4f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(facecolor = 'gray', loc = 'best')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_conf_matr(conf_matr, classes,\n",
    "                          normalize = False,\n",
    "                          title = 'Confusion matrix',\n",
    "                          cmap = plt.cm.winter):\n",
    "  \"\"\"\n",
    "  Citation\n",
    "  ---------\n",
    "  http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "  \"\"\"  \n",
    "  import itertools\n",
    "\n",
    "  accuracy = np.trace(conf_matr) / np.sum(conf_matr).astype('float')\n",
    "  sns.set(font_scale = 1.4)\n",
    "\n",
    "  plt.figure(figsize = (12, 8))\n",
    "  plt.imshow(conf_matr, interpolation = 'nearest', cmap = cmap)\n",
    "  title = '\\n' + title + '\\n'\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "\n",
    "  if classes is not None:\n",
    "      tick_marks = np.arange(len(classes))\n",
    "      plt.xticks(tick_marks, classes, rotation = 45)\n",
    "      plt.yticks(tick_marks, classes)\n",
    "\n",
    "  if normalize:\n",
    "      conf_matr = conf_matr.astype('float') / conf_matr.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "  thresh = conf_matr.max() / 1.5 if normalize else conf_matr.max() / 2\n",
    "  for i, j in itertools.product(range(conf_matr.shape[0]), range(conf_matr.shape[1])):\n",
    "      if normalize:\n",
    "          plt.text(j, i, \"{:0.2f}%\".format(conf_matr[i, j] * 100),\n",
    "                    horizontalalignment = \"center\",\n",
    "                    fontweight = 'bold',\n",
    "                    color = \"white\" if conf_matr[i, j] > thresh else \"black\")\n",
    "      else:\n",
    "          plt.text(j, i, \"{:,}\".format(conf_matr[i, j]),\n",
    "                    horizontalalignment = \"center\",\n",
    "                    fontweight = 'bold',\n",
    "                    color = \"white\" if conf_matr[i, j] > thresh else \"black\")\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label\\n\\nAccuracy = {:0.2f}%; Error = {:0.2f}%'.format(accuracy * 100, (1 - accuracy) * 100))\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_words(set, number):\n",
    "  words_counter = collections.Counter([word for sentence in set for word in sentence.split()]) # finding words along with count\n",
    "  most_counted = words_counter.most_common(number)\n",
    "  most_count = pd.DataFrame(most_counted, columns = [\"Words\", \"Amount\"]).sort_values(by = \"Amount\") # sorted data frame\n",
    "  most_count.plot.barh(x = \"Words\", \n",
    "                       y = \"Amount\",\n",
    "                       color = \"blue\",\n",
    "                       figsize = (10, 15))\n",
    "  for i, v in enumerate(most_count[\"Amount\"]):\n",
    "    plt.text(v, i,\n",
    "             \" \" + str(v),\n",
    "             color = 'black',\n",
    "             va = 'center',\n",
    "             fontweight = 'bold')\n",
    "\n",
    "def word_cloud(tag):\n",
    "  df_words_nl = ' '.join(list(df_spam[df_spam['feature'] == tag]['message']))\n",
    "  df_wc_nl = WordCloud(width = 600, height = 512).generate(df_words_nl)\n",
    "  plt.figure(figsize = (13, 9), facecolor = 'k')\n",
    "  plt.imshow(df_wc_nl)\n",
    "  plt.axis('off')\n",
    "  plt.tight_layout(pad = 1)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmrZguMTThd"
   },
   "source": [
    "## Reading the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvMGO3m4TThd"
   },
   "source": [
    "The files contain one message per line. Each line consists of two columns: v1 contains the label (`ham` or `spam`) and v2 contains the raw text. SMS spam (sometimes called cell phone spam) is any junk message delivered to a mobile phone as a text messaging through the Short Message Service (SMS). The practice is fairly rare in `North America` but has been common in `Japan` for years.\n",
    "\n",
    "***\n",
    "\n",
    "In this section you will read our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tW2bcLSr86z"
   },
   "outputs": [],
   "source": [
    "df_spam = pd.read_csv('spam.csv', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViO_fR3jW7U4"
   },
   "source": [
    "## Dataset manipulations & simple EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i11NJD1vKIv3"
   },
   "source": [
    "To make the columns(v1, v2) easy to read, we can rename them respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNff39M3D6bj"
   },
   "outputs": [],
   "source": [
    "df_spam = df_spam.filter(['v1', 'v2'], axis = 1)\n",
    "df_spam.columns = ['feature', 'message']\n",
    "df_spam.drop_duplicates(inplace = True, ignore_index = True)\n",
    "print('Number of null values:\\n')\n",
    "df_spam.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tk15rKDcE1W"
   },
   "source": [
    "Total ham(0) and spam(1) messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ikPFh_cb5m5"
   },
   "outputs": [],
   "source": [
    "df_spam['feature'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0ILuy9O0lOj"
   },
   "source": [
    "## Dataset size & feature names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do_MtuN_0mmQ"
   },
   "outputs": [],
   "source": [
    "df_spam.shape, df_spam.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DBf0FS70v_t"
   },
   "source": [
    "The dataset contains a lot of objects (rows), including 1 target feature (`feature`) and an additional column (`message`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SDpd1dxKeq4"
   },
   "source": [
    "Input features (column names):\n",
    "\n",
    "1.  `feature` - tags in this data collection\n",
    "2.  `message` - raw test message example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNsfcC39IHvj"
   },
   "source": [
    "Let's describe the data in a transposed way using both `describe` & `T` methods. The number of statistical output parameters from the data set is determined by the `describe` method.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wetTGQ5XfCLD"
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##\n",
    "df_spam.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXCXfDbqS3S0"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "df_spam.describe().T\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUmHnJn8JWU2"
   },
   "source": [
    "## Dataset primary statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqE5bdPIzB26"
   },
   "source": [
    "Let’s plot the number of value of both `spam` and `ham` messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_J-nCmN3bYry"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "counter = df_spam.shape[0]\n",
    "ax1 = sns.countplot(df_spam['feature'])\n",
    "ax2 = ax1.twinx()                      # Make double axis\n",
    "ax2.yaxis.tick_left()                 # Switch so the counter's axis is on the right, frequency axis is on the left\n",
    "ax1.yaxis.tick_right()\n",
    "ax1.yaxis.set_label_position('right')  # Also switch the labels over\n",
    "ax2.yaxis.set_label_position('left')\n",
    "ax2.set_ylabel('frequency, %')\n",
    "\n",
    "\n",
    "for p in ax1.patches:\n",
    "  x = p.get_bbox().get_points()[:, 0]\n",
    "  y = p.get_bbox().get_points()[1, 1]\n",
    "  ax1.annotate('{:.2f}%'.format(100. * y / counter),\n",
    "              (x.mean(), y),\n",
    "              ha = 'center',\n",
    "              va = 'bottom')\n",
    "\n",
    "# Use a LinearLocator to ensure the correct number of ticks\n",
    "ax1.yaxis.set_major_locator(ticker.LinearLocator(11))\n",
    "\n",
    "# Fix the frequency range to 0-100\n",
    "ax2.set_ylim(0, 100)\n",
    "ax1.set_ylim(0, counter)\n",
    "\n",
    "# And use a MultipleLocator to ensure a tick spacing of 10\n",
    "ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "\n",
    "# Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\n",
    "ax2.grid(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMQePIb3zNOf"
   },
   "source": [
    "The number of `ham` messages is almost for times bigger than that of `spam` messages in the data.\n",
    "\n",
    "Let’s plot the `number` (you can choose a number of words yourself in the range of `[5 .. 50]` (it should be divisible by `5`)) of different most often used words (while we can name these objects as words) present in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsbpkFt1jwKY"
   },
   "outputs": [],
   "source": [
    "plot_words(df_spam['message'], number = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-RbrItf0IKS"
   },
   "source": [
    "As you can see, the most often used words are `stopwords`. So we need to perform some preprocessing techniques on the dataset (see below [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01) substep in `Stage I`).\n",
    "\n",
    "Let's build the `WordCloud` image for the `spam` and the existed words (label `ham`) separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2JlIIE-QU99"
   },
   "outputs": [],
   "source": [
    "word_cloud('spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FctoedNrek4"
   },
   "outputs": [],
   "source": [
    "word_cloud('ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hGavnMC7wwG"
   },
   "source": [
    "## Part A. Advanced Machine Learning for SPAM classification task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W9uHiZxfpHt"
   },
   "source": [
    "### I stage. Preliminary actions. Preparing of needed sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMhnVon3gDJo"
   },
   "source": [
    "We need to define some input parameters for our next research, such as the `size of vocabulary`, sizes of `test` & `validation` sets, `dropping level`, etc. You can change some of those numerical parameters yourself (only where you can see the additional comments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWmehP3o54Tp"
   },
   "outputs": [],
   "source": [
    "size_vocabulary = 1000    # You can choose the size of vocabulary yourself in the range [500 .. 1500] (it should be divisible by 500)\n",
    "embedding_dimension = 64  # You can choose the size of dimention yourself in the range [32 .. 256] (it should be divisible by 32)\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "threshold = 0.5           # You can choose the size of threshold yourself in the range [0 .. 1]\n",
    "oov_token = \"<OOV>\"\n",
    "test_size, valid_size = 0.05, 0.2\n",
    "num_epochs = 20           # You can choose the number of epochs yourself in the range [20 .. 50] (it should be divisible by 5)\n",
    "drop_level = 0.3          # You can choose the size of drop level yourself in the range [0 .. 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXLkHz21TGk0"
   },
   "source": [
    "Next actions allow you to make data cleaning step by step, which consists of the following replace rules:\n",
    "\n",
    "1.  email addresses with 'emailaddr';\n",
    "2.  URLs with 'httpaddr';\n",
    "3.  money symbols with 'moneysymb';\n",
    "4.  phone numbers with 'phonenumbr';\n",
    "5.  numbers with 'numbr';\n",
    "6.  remove all punctuations;\n",
    "7.  word to lower case.\n",
    "\n",
    "Moreover, we make a `lemmatization` which is a method of morphological analysis. It comes down to reducing a word form to its initial dictionary form (lemma). As a result of  word forms lemmatization, flexive endings are discarded and the main or dictionary form of the word is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq6v0UsU2qFC"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage I. Preliminary actions. Preparing of needed sets\\n\")\n",
    "full_df_l = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(df_spam.shape[0]):\n",
    "    mess_1 = df_spam.iloc[i, 1]\n",
    "    mess_1 = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', mess_1)\n",
    "    mess_1 = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', mess_1) \n",
    "    mess_1 = re.sub('£|\\$', 'moneysymb', mess_1) \n",
    "    mess_1 = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', mess_1) \n",
    "    mess_1 = re.sub('\\d+(\\.\\d+)?', 'numbr', mess_1) \n",
    "    mess_1 = re.sub('[^\\w\\d\\s]', ' ', mess_1) \n",
    "    mess_1 = re.sub('[^A-Za-z]', ' ', mess_1).lower() \n",
    "    token_messages = word_tokenize(mess_1)\n",
    "    mess = []\n",
    "    for word in token_messages:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            mess.append(lemmatizer.lemmatize(word))\n",
    "    txt_mess = \" \".join(mess)\n",
    "    full_df_l.append(txt_mess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK6G6VORKdOV"
   },
   "source": [
    "Now, let’s plot the count words (`number` - you can choose a number of words yourself in the range `[5 .. 50]` (it should be divisible by `5`)) once again to see the most frequent words (without any stopwords, thus after all cleaning stages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk7rRyWHIQMA"
   },
   "outputs": [],
   "source": [
    "plot_words(full_df_l, number = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq8GUmqpLK0C"
   },
   "source": [
    "We can see that most common words are different from the stopwords. In addition, you can compare this picture with the result in the chapter `\"Dataset primary statistics\"`.\n",
    "\n",
    "Then the primary `df_spam` set will split into sentences (messages) and labels separately. Then we will split the full primary `df_spam` set with the following proportions: a training set (`75%`) and a test set (`25%`). Thus we will obtain 4 sets: two for sentences and two for labels with the same proportions.\n",
    "\n",
    "Also, we will do the `vectorization` with the help of `CountVectorizer` method. This is an easy way to make a collection of text documents and create a dictionary of famous words. This method converts the input text to the matrix, the values of which are the numbers of this key entry (words) in the text. Unfortunately, `FeatureHasher` has more configurable parameters (for example, you can set the tokenizer), but it works slowlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKce95T6xFVi"
   },
   "outputs": [],
   "source": [
    "add_df = CountVectorizer(max_features = size_vocabulary)\n",
    "X = add_df.fit_transform(full_df_l).toarray()\n",
    "y = df_spam.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = (test_size + valid_size), random_state = seed)\n",
    "print('Number of rows in test set: ' + str(X_test.shape))\n",
    "print('Number of rows in training set: ' + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_m40uGV_ojI"
   },
   "source": [
    "### II stage. Naive Bayes Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq05t5xJv9rJ"
   },
   "source": [
    "Let's find a set of predictions based on our models: `Guassian Naive Bayes` and `Multinomial Naive Bayes`. In addition, we will build a `classification report` and draw the `confusion matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSHZaZ9o-Ucw"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage IIa. Guassian Naive Bayes\\n\")\n",
    "class_NBC = GaussianNB().fit(X_train, y_train) # Guassian Naive Bayes\n",
    "y_pred_NBC = class_NBC.predict(X_test)\n",
    "print('The first two predicted labels:', y_pred_NBC[0],y_pred_NBC[1], '\\n')\n",
    "conf_m_NBC = confusion_matrix(y_test, y_pred_NBC)\n",
    "class_rep_NBC = classification_report(y_test, y_pred_NBC)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_NBC, '\\n')\n",
    "plot_conf_matr(conf_m_NBC, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for Guassian Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7A28siI-o0Z"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage IIb. Multinomial Naive Bayes\\n\")\n",
    "class_MNB = MultinomialNB().fit(X_train, y_train) # Multinomial Naive Bayes\n",
    "y_pred_MNB = class_MNB.predict(X_test)\n",
    "print('The first two predicted labels:', y_pred_MNB[0],y_pred_MNB[1], '\\n')\n",
    "conf_m_MNB = confusion_matrix(y_test, y_pred_MNB)\n",
    "class_rep_MNB = classification_report(y_test, y_pred_MNB)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_MNB, '\\n')\n",
    "plot_conf_matr(conf_m_MNB, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for Multinomial Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsHtFKWVgnh3"
   },
   "source": [
    "### III stage. Decision Tree Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zv_yCjzgx1_"
   },
   "source": [
    "Let's find a set of predictions based on our `Decision Tree Classifier` model. In addition, we will build a `classification report` and draw the `confusion matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8coEIzwnw7My"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage III. Decision Tree Classifier\\n\")\n",
    "class_DTC = DecisionTreeClassifier(random_state = seed).fit(X_train, y_train)\n",
    "y_pred_DTC = class_DTC.predict(X_test)\n",
    "print('The first two predicted labels:', y_pred_DTC[0], y_pred_DTC[1], '\\n')\n",
    "conf_m_DTC = confusion_matrix(y_test, y_pred_DTC)\n",
    "class_rep_DTC = classification_report(y_test, y_pred_DTC)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_DTC, '\\n')\n",
    "plot_conf_matr(conf_m_DTC, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xtc4cpDuhlLh"
   },
   "source": [
    "### IV stage. Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlREINxVivQH"
   },
   "source": [
    "Let's find a set of predictions based on our `Logistic Regression` model. In addition, we will build a `classification report` and draw the `confusion matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nf6e4GbwbTF"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage IV. Logistic Regression\\n\")\n",
    "class_LR = LogisticRegression(random_state = seed, solver = 'liblinear').fit(X_train, y_train)\n",
    "y_pred_LR = class_LR.predict(X_test)\n",
    "print('The first two predicted labels:', y_pred_LR[0], y_pred_LR[1], '\\n')\n",
    "conf_m_LR = confusion_matrix(y_test, y_pred_LR)\n",
    "class_rep_LR = classification_report(y_test, y_pred_LR)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_LR, '\\n')\n",
    "plot_conf_matr(conf_m_LR, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isHa47x7jaGe"
   },
   "source": [
    "### V stage. KNeighbors Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2xcvn2EjhNQ"
   },
   "source": [
    "Let's find a set of predictions based on our `KNeighbors Classifier` model. In addition, we will build a `classification report` and draw the `confusion matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv7gR5653034"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage V. KNeighbors Classifier\\n\")\n",
    "class_KNC = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)\n",
    "y_pred_KNC = class_KNC.predict(X_test)\n",
    "print('The firs two predicted labels:', y_pred_KNC[0], y_pred_KNC[1], '\\n')\n",
    "conf_m_KNC = confusion_matrix(y_test, y_pred_KNC)\n",
    "class_rep_KNC = classification_report(y_test, y_pred_KNC)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_KNC, '\\n')\n",
    "plot_conf_matr(conf_m_KNC, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for KNeighbors Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1cLq05slA_U"
   },
   "source": [
    "### VI stage. Support Vector Classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x20jxggVqgGl"
   },
   "source": [
    "Let's find a set of predictions based on our `Support Vector Classification` model. In addition, we will build a `classification report` and draw the `confusion matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awr9mi4Aztqc"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage VI. Support Vector Classification\\n\")\n",
    "class_SVC = SVC(probability = True, random_state = seed).fit(X_train, y_train)\n",
    "y_pred_SVC = class_SVC.predict(X_test)\n",
    "print('The first two predicted labels:', y_pred_SVC[0], y_pred_SVC[1], '\\n')\n",
    "conf_m_SVC = confusion_matrix(y_test, y_pred_SVC)\n",
    "class_rep_SVC = classification_report(y_test, y_pred_SVC)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_SVC, '\\n')\n",
    "plot_conf_matr(conf_m_SVC, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for SVC Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyRjbH0EyOyT"
   },
   "source": [
    "### VII stage. Gradient Boosting Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKeW3viryrSC"
   },
   "source": [
    "Let's find a set of predictions based on our `Gradient Boosting Classifier` model. In addition, we will build a `classification report` and draw the `confusion matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFp4lthL5Sqq"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage VII. Gradient Boosting Classifier\\n\")\n",
    "class_GBC = GradientBoostingClassifier(random_state = seed).fit(X_train, y_train)\n",
    "y_pred_GBC = class_GBC.predict(X_test)\n",
    "print('The first two predicted labels:', y_pred_GBC[0], y_pred_GBC[1], '\\n')\n",
    "conf_m_GBC = confusion_matrix(y_test, y_pred_GBC)\n",
    "class_rep_GBC = classification_report(y_test, y_pred_GBC)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_GBC, '\\n')\n",
    "plot_conf_matr(conf_m_GBC, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for Gradient Boosting Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWCxHx2K23gS"
   },
   "source": [
    "### VIII stage. Bagging Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSYuFkZe276A"
   },
   "source": [
    "Let's find a set of predictions based on our `Bagging Classifier` model. In addition, we will build a `classification report` and draw the `confusion matrix`.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnD4msd55_7x"
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tStage VIII. Bagging Classifier + something else\\n\")\n",
    "class_BC = BaggingClassifier(class_SVC).fit(X_train, y_train)\n",
    "y_pred_BC = class_BC.predict(X_test)\n",
    "print('The first two predicted labels:', y_pred_BC[0], y_pred_BC[1], '\\n')\n",
    "conf_m_BC = confusion_matrix(y_test, y_pred_BC)\n",
    "class_rep_BC = classification_report(y_test, y_pred_BC)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_BC, '\\n')\n",
    "plot_conf_matr(conf_m_BC, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for Bagging Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knvGS1HyA_NZ"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "class_BC = BaggingClassifier(class_SVC).fit(X_train, y_train)\n",
    "# OR\n",
    "class_BC = BaggingClassifier(class_DTC).fit(X_train, y_train)\n",
    "# OR\n",
    "class_BC = BaggingClassifier(class_KNC).fit(X_train, y_train)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-V-aNIYR5j8"
   },
   "source": [
    "Moreover, as you can see, the `Bugging Classifier` can work with some different classifiers as with basic ones, such as `SVC`, `KNC`, `DTC`, etc. In this case, the main purpose of its usage is to increase the accuracy obtained earlier from the basic classifier. You can check this fact comparing, for instance, the obtained earlier `SVC classifier` accuracy and the accuracy after using `Bugging Classifier` with `SVC`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3k94brgQ662"
   },
   "source": [
    "## Part B. Advanced Deep Learning for SPAM classification task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNxTK61jmSVE"
   },
   "source": [
    "### I stage. Preliminary actions. Preparing of needed sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w1U2WS4Xqn4"
   },
   "source": [
    "We need to prepare our sets to the new DL model for SPAM classification task, such as training, validation & test sets based on the primary `df_spam` set. We need a training set for training a pre-built model, a validation set is used for finding better hyperparameters, a test set will be used for checking our trained model on data which the model didn't see.\n",
    "\n",
    "Firstly, the primary `df_spam` set will split into sentences (messages) and labels separately. Then we will split the full primary `df_spam` set with the following proportions: a training set (`75%`), a validation set (`20%`) and a test set (`5%`). Thus we will obtain 6 sets: three for sentences and three for labels with the same proportions.\n",
    "\n",
    "Nevertheless, you can change those proportions in percentages as you see it. However, you should remember that these changes can influence your model accuracy, and as it often happens, they decrease it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSu11mHe7xKh"
   },
   "outputs": [],
   "source": [
    "print(\"Stage I. Preliminary actions. Preparing of needed sets\\n\")\n",
    "\n",
    "sentences_new_set = []\n",
    "labels_new_set = []\n",
    "for i in range(0, df_spam.shape[0], 1):\n",
    "    sentences_new_set.append(df_spam['message'][i])\n",
    "    labels_new_set.append(df_spam['feature'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MTEcbue7jW3"
   },
   "outputs": [],
   "source": [
    "train_size = int(df_spam.shape[0] * (1 - test_size - valid_size))\n",
    "valid_bound = int(df_spam.shape[0] * (1 - valid_size))\n",
    "\n",
    "train_sentences = sentences_new_set[0 : train_size]\n",
    "valid_sentences = sentences_new_set[train_size : valid_bound]\n",
    "test_sentences = sentences_new_set[valid_bound : ]\n",
    "\n",
    "train_labels_str = labels_new_set[0 : train_size]\n",
    "valid_labels_str = labels_new_set[train_size : valid_bound]\n",
    "test_labels_str = labels_new_set[valid_bound : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTbsPsQtlsHA"
   },
   "source": [
    "### II stage. Labels transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtiJn-Phl6G3"
   },
   "source": [
    "Secondly, we will replace all the labels (with the following values: `ham` and `spam`) to the appropriate values `1` and `0`, and transform them to Numpy arrays.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us_2pYwDEG6p"
   },
   "outputs": [],
   "source": [
    "print(\"Stage II. Labels transformations\\n\")\n",
    "\n",
    "train_labels = [0] * len(train_labels_str)\n",
    "for ind, item in enumerate(train_labels_str):\n",
    "    if item == 'ham':\n",
    "        train_labels[ind] = 1\n",
    "    else:\n",
    "        train_labels[ind] = 0\n",
    "        \n",
    "valid_labels = [0] * len(valid_labels_str)\n",
    "for ind, item in enumerate(valid_labels_str):\n",
    "    if item == 'ham':\n",
    "        valid_labels[ind] = 1\n",
    "    else:\n",
    "        valid_labels[ind] = 0\n",
    "\n",
    "test_labels = [0] * len(test_labels_str)\n",
    "for ind, item in enumerate(test_labels_str):\n",
    "    if item == 'ham':\n",
    "        test_labels[ind] = 1\n",
    "    else:\n",
    "        test_labels[ind] = 0\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "valid_labels = np.array(valid_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uephw9xHnMaB"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "train_labels = np.array(train_labels)\n",
    "valid_labels = np.array(valid_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuIE6kGFl-0c"
   },
   "source": [
    "### III stage. Tokenization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TMAplfUYYiY"
   },
   "source": [
    "[Tokenization](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01) is a process of splitting up a large body of text into smaller lines or words. It helps in interpreting the meaning of the text by analyzing the sequence of the words. We converted our output feature into a numerical form, then, what about the input feature based on `size_vocabulary`.\n",
    "\n",
    "First, let’s tokenize our data and convert it into a numerical sequence using `Keras` `Tokenizer`. We can also find the index number `word_index` of the corresponding words. We will need a really big word index to handle sentences that are not in the training set. This can be handled using the `Out Of Vocabulary` <OOV> token variable `oov_token`.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ud1ti0JZ4VOQ"
   },
   "outputs": [],
   "source": [
    "print(\"Stage III. Tokenization\\n\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words = size_vocabulary,\n",
    "                      oov_token = oov_token,\n",
    "                      lower = False)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHGfHBZ8n01h"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "tokenizer = Tokenizer(num_words = size_vocabulary,\n",
    "                      oov_token = oov_token,\n",
    "                      lower = False)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5iEqIQDXTuz"
   },
   "source": [
    "As you can see in `text_to_sequence`, all the sequences are of different lengths which are not compatible for the model to train. So we should make all the sentences length equal. For this, we are padding the sequences with `padding_type`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQ0kJNTaFOkW"
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "size_voc = len(word_index) + 1\n",
    "max_len = max([len(i) for i in train_sequences])\n",
    "train_set = pad_sequences(train_sequences,\n",
    "                                padding = padding_type,\n",
    "                                maxlen = max_len,\n",
    "                                truncating = trunc_type) \n",
    "\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
    "valid_set = pad_sequences(valid_sequences,\n",
    "                               padding = padding_type,\n",
    "                               maxlen = max_len,\n",
    "                               truncating = trunc_type)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_set = pad_sequences(test_sequences,\n",
    "                               padding = padding_type,\n",
    "                               maxlen = max_len,\n",
    "                               truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TDZGntCmgEg"
   },
   "source": [
    "### IV stage. Model building.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-yMDx6jpnPQ"
   },
   "source": [
    "You should create your own model at this stage.\n",
    "\n",
    "The first layer of the model is `Embedding` layer, which can be used to create `dense` encoding of words based on an input `size_voc` of defined vocabulary (in our case it's the index number `word_index` of the corresponding words `+ 1`). Typically sparse and dense word encodings denote coding efficiency.\n",
    "\n",
    "Further, we use one (you can change this number) pair of layers: `Dense` & `Dropout`. You can choose a number of layers pairs yourself.\n",
    "\n",
    "Using `bidirectional LSTM` will run your input in two ways: one from the past to the future and one from the future to the past (in a back way). This distinguishes this approach from `unidirectional LSTM` which works in the opposite direction, so you save information from the future. Thus, by using the two hidden states together, you can save information from both the past and the future at any time.\n",
    "\n",
    "`Dropout` [layer](https://arxiv.org/abs/1207.0580?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01) is used in neural networks to solve the problem of overfitting. Networks for training are obtained by dropping out neurons with probability `p`, so the probability that a neuron will remain in the network is `1 - p`.\n",
    "\n",
    "`Dense` layer is an ordinary tightly bonded layer of a neural network where each neuron is connected to all inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpxA3CnKExqI"
   },
   "outputs": [],
   "source": [
    "print(\"Stage IV. Model building\\n\")\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(size_voc, embedding_dimension, input_length = max_len),\n",
    "    Bidirectional(LSTM(100)),\n",
    "    Dropout(drop_level),\n",
    "    Dense(20, activation = 'relu'),\n",
    "    Dropout(drop_level),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPpsuDGxqlQw"
   },
   "source": [
    "### V stage. Model compiling & fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxaTtocvqtVK"
   },
   "source": [
    "This stage allows you to train your model, but firstly, you should set some hyperparameters & other variables values, such as `batch size`, number of `epochs` for training, types of `optimizer` & `loss` function. You can change all or a part of them during your research.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9elKURGeKNEY"
   },
   "outputs": [],
   "source": [
    "print(\"Stage V. Model compiling & fitting\\n\")\n",
    "optim = Adam(learning_rate = 0.0001)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = optim,\n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87HdLEufrIan"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = optim,\n",
    "              metrics = ['accuracy'])\n",
    "model.summary()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dokKMxDKVka"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_set, \n",
    "                    train_labels,\n",
    "                    epochs = num_epochs, \n",
    "                    validation_data = (valid_set, valid_labels),\n",
    "                    workers = os.cpu_count(),\n",
    "                    use_multiprocessing = True,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkCbKquXrHGA"
   },
   "source": [
    "### VI stage. Results visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LqgnPp8riIZ"
   },
   "source": [
    "You see the results of training for the loss & accuracy.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92dVg22VN09V"
   },
   "outputs": [],
   "source": [
    "print(\"Stage VI. Results visualization\\n\")\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08pGS8qwrrL8"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "plot_history(history)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj0qcj6YsTPE"
   },
   "source": [
    "If you can see the values reduction for the `loss` distribution, and if you see the values increase for the `accuracy`, then it's a good sign. It means your model training goes in the right direction.\n",
    "\n",
    "Thus, the main goal has been reached.\n",
    "\n",
    "In addition, let's estimate your pre-built model on the test set which this model hasn't seen in any case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmpsTeb4ZjRE"
   },
   "outputs": [],
   "source": [
    "model_score = model.evaluate(test_set, test_labels, batch_size = embedding_dimension, verbose = 1)\n",
    "print(f\"Test accuracy: {model_score[1] * 100:0.2f}% \\t\\t Test error: {model_score[0]:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XVQTcrzs0jJ"
   },
   "source": [
    "### VII stage. Model saving & predict checking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9ZuHEvdXBQU"
   },
   "source": [
    "We can save our model and tokenizer for future uses in different formats. We have to do two more steps: to save our trained model so that we can use it in the further research. In addition, we should check our saved model and try to make a forecast.\n",
    "\n",
    "You should enter any name of your model for saving but make sure that it is in quotes.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuRBSGZITAi7"
   },
   "outputs": [],
   "source": [
    "M_name = \"My_model\"\n",
    "pickle.dump(tokenizer, open(M_name + \".pkl\", \"wb\"))\n",
    "filepath = M_name + '.h5'\n",
    "tf.keras.models.save_model(model, filepath, include_optimizer = True, save_format = 'h5', overwrite = True)\n",
    "print(\"Size of the saved model :\", os.stat(filepath).st_size, \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLRR9TMttHi8"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "M_name = \"My_model\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9vAYG79tdco"
   },
   "source": [
    "Let's find a set of predictions based on our model. We will enter the `threshold` value (`0.5`) which will help us to mark correctly and incorrectly predicted labels. In addition, we will build a `classification report` (as for the previous studied ML models - see `Part A`) and draw the `confusion matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdLobcYSS26B"
   },
   "outputs": [],
   "source": [
    "y_pred_bLSTM = model.predict(test_set)\n",
    "\n",
    "y_prediction = [0] * y_pred_bLSTM.shape[0]\n",
    "for ind, item in enumerate(y_pred_bLSTM):\n",
    "    if item > threshold:\n",
    "        y_prediction[ind] = 1\n",
    "    else:\n",
    "        y_prediction[ind] = 0\n",
    "\n",
    "conf_m_bLSTM = confusion_matrix(test_labels, y_prediction)\n",
    "class_rep_bLSTM = classification_report(test_labels, y_prediction)\n",
    "print('\\t\\t\\tClassification report:\\n\\n', class_rep_bLSTM, '\\n')\n",
    "plot_conf_matr(conf_m_bLSTM, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for bLSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svOPMnE4Qfxm"
   },
   "source": [
    "Let's check our trained model on the real messages which you can create yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89MdFnIxG2Hb"
   },
   "outputs": [],
   "source": [
    "# You can change this message (as any short sentence) yourself\n",
    "message_example = [\"Darling, please give me a cup of tea\"] \n",
    "\n",
    "message_example_tp = pad_sequences(tokenizer.texts_to_sequences(message_example),\n",
    "                                   maxlen = max_len,\n",
    "                                   padding = padding_type,\n",
    "                                   truncating = trunc_type)\n",
    "\n",
    "pred = float(model.predict(message_example_tp))\n",
    "if (pred > threshold):\n",
    "    print (\"This message is a real text\")\n",
    "else:\n",
    "    print(\"This message is a spam message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsKaBIr0XFmN"
   },
   "source": [
    "Through this article, you will be able to understand and create a text classification model using LSTM architecture. In future articles, we will see other text classification techniques and other Natural Langauge Processing models.\n",
    "\n",
    "Moreover, you have learnt both the Machine Learning (ML) and Deep Learning (DL) models to solve the SPAM classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEbNXiqmTTh0"
   },
   "source": [
    "## Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_1ZOvXKTTh0"
   },
   "source": [
    "[Sergii Kavun](https://www.linkedin.com/in/sergii-kavun/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsAdvanced_ML_DL_spam_classification_L427910497-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHpM3GyTTTh2"
   },
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrgMUa_ETTh2"
   },
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By    | Change Description   |\n",
    "| ----------------- | ------- | ------------- | -------------------- |\n",
    "| 2021-07-11        | 1.0     | Kavun, Sergii | Code improving       |\n",
    "| 2021-07-06        | 0.21    | Kavun, Sergii | Code refactoring     |\n",
    "| 2021-07-03        | 0.2     | Kavun, Sergii | Translate to english |\n",
    "| 2021-07-01        | 0.1     | Kavun, Sergii | Created Lab          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma5EMufCTTh2"
   },
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Advanced_ML_DL_spam_classification_L4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
